ğŸ“˜ Teaching Assistant Chatbot (Multimodal RAG)

A local, multimodal Retrieval-Augmented Generation (RAG) chatbot that acts as a school teaching assistant, capable of answering questions from large textbook PDFs containing text, tables, and imagesâ€”fully offline using Ollama + LLaVA.

ğŸ“‘ Table of Contents

Overview

Key Features

Architecture & Design

Tech Stack

Project Structure

Installation & Setup

Usage

Workflow / Pipeline Explanation

Configuration

Limitations & Known Issues

Future Improvements

Why This Design?

Contributing

License

ğŸ“– Overview

This project is a Teaching Assistant Chatbot built using a multimodal RAG architecture.

It allows students to ask questions from real school textbooks (PDFs with 800+ pages) and receive accurate answers derived from:

Text explanations

Tables

Diagrams and images

Real-World Use Case

Digital classroom assistant

Self-study companion for students

Offline AI tutor for schools with limited internet access

Why This Is Non-Trivial

Handles very large PDFs

Supports multimodal reasoning

Runs entirely locally (no APIs)

Memory-efficient ingestion and retrieval

âœ¨ Key Features

ğŸ“š Supports multiple large PDFs

ğŸ§  Multimodal RAG (Text + Tables + Images)

âš¡ Memory-efficient, page-wise ingestion

ğŸ—‚ Persistent vector store (ChromaDB)

ğŸ” Context-aware retrieval (Top-K)

ğŸ–¼ Image reasoning via LLaVA

ğŸ” Fully offline & privacy-safe

ğŸ§© Modular, production-ready design

ğŸ— Architecture & Design
System Flow

PDFs are loaded page-by-page

Content is split into:

Text

Tables

Images

Each modality is summarized

Summaries are embedded

Vectors are stored in ChromaDB

User query retrieves relevant chunks

LLaVA reasons over:

Text & tables

Images (if relevant)

Final answer is generated

Why This Architecture?

Avoids loading entire PDFs into memory

Summarization reduces token cost

Multimodal RAG reflects real textbooks

Local-first for cost and privacy

ğŸ§° Tech Stack
Backend

Python

LangChain

AI / ML

Ollama

LLaVA (7B)

OllamaEmbeddings

Document Processing

Unstructured (partition_pdf)

Vector Store

ChromaDB

Frontend

Streamlit

Tools

dotenv

uuid

base64

ğŸ“‚ Project Structure
.
â”œâ”€â”€ ingestion.py          # PDF loading, parsing & summarization
â”œâ”€â”€ embeddings.py         # Embedding + vector store creation
â”œâ”€â”€ rag.py                # Multimodal RAG logic
â”œâ”€â”€ prompts.py            # Centralized prompt templates
â”œâ”€â”€ streamlit_app.py      # UI for chatbot
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ book1.pdf
â”‚   â”œâ”€â”€ book2.pdf
â”‚   â””â”€â”€ images/           # Extracted images
â”œâ”€â”€ chroma_db/            # Persistent vector store
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš™ï¸ Installation & Setup
Prerequisites

Python 3.10+

Ollama installed locally

Minimum 8 GB RAM (16 GB recommended)

Git

Step 1ï¸âƒ£ Clone the Repository
git clone https://github.com/your-org/teaching-assistant-chatbot.git
cd teaching-assistant-chatbot

Step 2ï¸âƒ£ Create & Activate Virtual Environment
python -m venv venv


Linux / macOS

source venv/bin/activate


Windows

venv\Scripts\activate

Step 3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt


No API keys required.

Step 4ï¸âƒ£ Install Ollama

Download and install from:
https://ollama.com

Step 5ï¸âƒ£ Pull Required Model
ollama pull llava:7b

â–¶ï¸ Usage
Step 1ï¸âƒ£ Add PDFs

Place PDFs in:

data/
 â”œâ”€â”€ book1.pdf
 â””â”€â”€ book2.pdf

Step 2ï¸âƒ£ Run Embedding Pipeline
python embeddings.py


This step:

Extracts content

Summarizes

Embeds

Stores vectors permanently

Run only once unless PDFs change.

Step 3ï¸âƒ£ Start the Chatbot UI
streamlit run streamlit_app.py


Open browser:

http://localhost:8501

Sample Input
Explain the process of photosynthesis using the diagram.

Sample Output

Clear explanation from textbook text

Additional insights from diagrams (if available)

ğŸ”„ Workflow / Pipeline Explanation
PDFs
 â†“
Page-wise Loading
 â†“
Text / Table / Image Separation
 â†“
Summarization
 â†“
Embedding
 â†“
ChromaDB
 â†“
Retriever
 â†“
Multimodal Prompting
 â†“
LLaVA Reasoning
 â†“
Answer

ğŸ”§ Configuration
Configurable Values

TOP_K â€“ number of retrieved chunks

COLLECTION_NAME

VECTOR_DIR

Ollama model (llava:7b)

Defined directly in code.

âš ï¸ Limitations & Known Issues

OCR quality depends on PDF scans

Image reasoning accuracy varies

No page citation yet

Initial ingestion may take time

ğŸš€ Future Improvements

Source/page references

Hybrid retrieval (BM25 + Vector)

Docker support

Advanced OCR

Chat history memory

Better UI annotations

ğŸ¤” Why This Design?
Decision	Reason
Local LLM	Privacy & zero cost
Summarize before embedding	Performance & recall
Multimodal RAG	Real textbook understanding
ChromaDB	Lightweight & persistent
ğŸ¤ Contributing

Fork the repo

Create a feature branch

Commit clean code

Open a Pull Request
